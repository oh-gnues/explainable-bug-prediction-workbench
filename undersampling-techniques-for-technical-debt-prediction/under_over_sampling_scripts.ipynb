{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4114599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These scripts apply undersampling techniques and oversampling techniques independently to an imbalanced TD dataset. \n",
    "# Additionally, they check whether synergy occurs when using both undersampling and oversampling techniques simultaneously. \n",
    "# To run these scripts, the imblearn package containing implementations of undersampling and oversampling techniques is required \n",
    "# and additional installation of scipy and cliffs_delta packages may be necessary for the statistical validation of cross-validations. \n",
    "# As we perform 100 rounds of 5-fold cross-validations to compare the performance of ML models on the original dataset and sampled datasets, \n",
    "# it may take a long time depending on the experimental environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172ca745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy, time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss, TomekLinks, EditedNearestNeighbours, NeighbourhoodCleaningRule, AllKNN\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN, KMeansSMOTE, SVMSMOTE\n",
    "\n",
    "from scipy.stats import wilcoxon\n",
    "from cliffs_delta import cliffs_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0f9774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset of Tsoukalas et al.\n",
    "X = pd.read_csv('./X.csv', sep=',')\n",
    "Y = pd.read_csv('./Y.csv', sep=',')\n",
    "X = MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa9048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML models in this study. no hyper parameter tuning for the fair comparison.\n",
    "lr = LogisticRegression(n_jobs=-1, random_state=0)\n",
    "svm = SVC(random_state=0)   \n",
    "rf = RandomForestClassifier(n_jobs=-1, random_state=0)\n",
    "xgb = XGBClassifier(n_jobs=-1, random_state=0)\n",
    "models = [lr, svm, rf, xgb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0aebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning the performance of each model and time to obtain the results \n",
    "def cross_validator(pipeline):\n",
    "    start=time.time()\n",
    "    score_measure = {'accuracy': 'accuracy', 'precision': 'precision', 'recall': 'recall', 'f1': 'f1', 'AUC': 'roc_auc'}\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=100, random_state=0)\n",
    "    scores = cross_validate(pipeline, X, Y.values.ravel(), scoring=score_measure, cv=cv, n_jobs=-1, error_score='raise')\n",
    "    accuracy = (np.mean(scores['test_accuracy']), np.std(scores['test_accuracy']))\n",
    "    precision = (np.mean(scores['test_precision']), np.std(scores['test_precision']))\n",
    "    recall = (np.mean(scores['test_recall']), np.std(scores['test_recall']))\n",
    "    f1 = (np.mean(scores['test_f1']), np.std(scores['test_f1']))\n",
    "    auc = (np.mean(scores['test_AUC']), np.std(scores['test_AUC']))\n",
    "    end = time.time()\n",
    "    return scores, (end-start)\n",
    "\n",
    "# pipelining for different configutations\n",
    "def give_me_pipe(over, under, model):\n",
    "    return [('over', over), ('under', under), ('model', model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9919cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the performance of ML models for original dataset\n",
    "lr_svm_rf_xgb = []\n",
    "for model in copy.deepcopy(models):\n",
    "    temp = cross_validator(Pipeline(give_me_pipe(None,None,model)))\n",
    "    scores = temp[0]\n",
    "    lr_svm_rf_xgb.append(temp)\n",
    "    accuracy = (np.mean(scores['test_accuracy']), np.std(scores['test_accuracy']))\n",
    "    precision = (np.mean(scores['test_precision']), np.std(scores['test_precision']))\n",
    "    recall = (np.mean(scores['test_recall']), np.std(scores['test_recall']))\n",
    "    f1 = (np.mean(scores['test_f1']), np.std(scores['test_f1']))\n",
    "# 500 F1-scores to compare \n",
    "lr_cv_100 = lr_svm_rf_xgb[0][0]['test_f1']\n",
    "svm_cv_100 = lr_svm_rf_xgb[1][0]['test_f1']\n",
    "rf_cv_100 = lr_svm_rf_xgb[2][0]['test_f1']\n",
    "xgb_cv_100 = lr_svm_rf_xgb[3][0]['test_f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2648b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example for applying undersampling or oversampling techniques exclusively.\n",
    "# This script trains four models on oversampled datasets with SMOTE applied at sampling rates\n",
    "# ranging from 0.1 to 1.0, and display their performance in four data frames.\n",
    "# In this script, you can evaluate all sampling techniques by simply changing the parameters\n",
    "# in the 'give_me_pipe' method.\n",
    "# The parameter options:\n",
    "# =============================================\n",
    "# Oversampling techniques\n",
    "# SMOTE(random_state=0, sampling_strategy=i)\n",
    "# BorderlineSMOTE(random_state=0, sampling_strategy=i)\n",
    "# ADASYN(random_state=0, sampling_strategy=i)\n",
    "# KMeansSMOTE(random_state=0, cluster_balance_threshold=0.03, sampling_strategy=i)\n",
    "# please, note that cluster_balance_threshold is set.\n",
    "# because the folds cannot be splitted as same ratio (0.03351688486) \n",
    "# as original dataset because of splitting, we used 0.03 for 5-fold cross validation.\n",
    "# SVMSMOTE(random_state=0, sampling_strategy=i)\n",
    "# =============================================\n",
    "# Undersampling techniques:\n",
    "# RandomUnderSampler(random_state=0)\n",
    "# NearMiss(version=3, n_jobs=-1)\n",
    "# AllKNN(n_jobs=-1)\n",
    "# EditedNearestNeighbours(n_jobs=-1)\n",
    "# TomekLinks(n_jobs=-1)\n",
    "# NeighbourhoodCleaningRule(n_jobs=-1)\n",
    "# =============================================\n",
    "# Applying oversampling and undersampling techniques simultaneously:\n",
    "# ex) give_me_pipe(SMOTE(random_state = 0, sampling_strategy=i), RandomUnderSampler(random_state=0), lr)\n",
    "for model in copy.deepcopy(models):\n",
    "    SMOTE_df = pd.DataFrame()\n",
    "    SMOTE_df.astype('float')\n",
    "    for i in list(np.arange(0.1, 1.01, 0.1)):\n",
    "        try:\n",
    "            pipe = Pipeline(\n",
    "                give_me_pipe(\n",
    "                    SMOTE(random_state=0, sampling_strategy=i), None,\n",
    "                    model))\n",
    "            temp = cross_validator(pipe)\n",
    "            scores = temp[0]\n",
    "            _time = temp[1]\n",
    "            accuracy = (np.mean(scores['test_accuracy']), np.std(scores['test_accuracy']))\n",
    "            precision = (np.mean(scores['test_precision']), np.std(scores['test_precision']))\n",
    "            recall = (np.mean(scores['test_recall']), np.std(scores['test_recall']))\n",
    "            f1 = (np.mean(scores['test_f1']), np.std(scores['test_f1']))\n",
    "            auc = (np.mean(scores['test_AUC']), np.std(scores['test_AUC']))\n",
    "            SMOTE_df = pd.concat([SMOTE_df, pd.DataFrame.from_records([{\n",
    "                'Accuracy':'%.3f' % accuracy[0],\n",
    "                'Precision':'%.3f' % precision[0],\n",
    "                'Recall':'%.3f' % recall[0],\n",
    "                'F1-score':'%.3f' % f1[0],\n",
    "                'AUC':'%.3f' % auc[0],\n",
    "                'TIME': '%.3f' % _time\n",
    "            }])], ignore_index=True)\n",
    "        except ValueError as v:\n",
    "            SMOTE_df = pd.concat([SMOTE_df,pd.DataFrame.from_records([{\n",
    "                'Accuracy':'%.3f' % 0,\n",
    "                'Precision':'%.3f' % 0,\n",
    "                'Recall':'%.3f' % 0,\n",
    "                'F1-score':'%.3f' % 0,\n",
    "                'AUC':'%.3f' % 0,\n",
    "                'TIME':'%.3f' % 0\n",
    "            }])], ignore_index=True)\n",
    "    SMOTE_df.set_index([pd.Index(list(np.arange(0.1, 1.01, 0.1)))], inplace=True)\n",
    "    print(\"model : \" + str(model))\n",
    "    display(SMOTE_df)\n",
    "    print(\"============================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dbc737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As shown in the results above, once the optimal results have been found\n",
    "# by tuning the sampling rate, perform cross-validation once more with the script below \n",
    "# to see if the same results can be achieved. \n",
    "# Afterwards, compare the performance of ML models on the sampled datasets with SMOTE\n",
    "# with the performance of ML models on the original dataset.\n",
    "# At this point, perform the Wilcoxon signed-rank test \n",
    "# and evaluate the Cliff's delta effect size. \n",
    "# The results of this script are generated as shown below, \n",
    "# and you can confirm that they are the same as Table 3 in the study. \n",
    "# Since it takes a very long time to check the results \n",
    "# when including scripts for all combinations, \n",
    "# we have provided the simplest scripts that can implement all of them. \n",
    "# The results of all combinations are included in the attached csv file.\n",
    "\n",
    "# SMOTE lr\n",
    "SMOTE_lr_cv_100 = []\n",
    "lr2 = copy.deepcopy(lr)\n",
    "SMOTE_lr_df = pd.DataFrame()\n",
    "SMOTE_lr_df.astype('float')\n",
    "SMOTE_lr_pipe = Pipeline(give_me_pipe(SMOTE(random_state = 0, sampling_strategy=0.2), None, lr2))\n",
    "temp = cross_validator(SMOTE_lr_pipe)\n",
    "SMOTE_lr_cv_100 = temp[0]['test_f1']\n",
    "scores = temp[0]\n",
    "accuracy = (np.mean(scores['test_accuracy']), np.std(scores['test_accuracy']))\n",
    "precision = (np.mean(scores['test_precision']), np.std(scores['test_precision']))\n",
    "recall = (np.mean(scores['test_recall']), np.std(scores['test_recall']))\n",
    "f1 = (np.mean(scores['test_f1']), np.std(scores['test_f1']))\n",
    "auc = (np.mean(scores['test_AUC']), np.std(scores['test_AUC']))\n",
    "SMOTE_lr_df = pd.concat([SMOTE_lr_df, pd.DataFrame.from_records([{\n",
    "    'Accuracy':'%.3f' % accuracy[0],\n",
    "    'Precision':'%.3f' % precision[0],\n",
    "    'Recall':'%.3f' % recall[0],\n",
    "    'F1-score':'%.3f' % f1[0],\n",
    "    'AUC':'%.3f' % auc[0],'TIME' : '%.3f'% temp[1]\n",
    "}])], ignore_index=True)\n",
    "display(SMOTE_lr_df)\n",
    "\n",
    "# SMOTE svm\n",
    "SMOTE_svm_cv_100 = []\n",
    "svm2 = copy.deepcopy(svm)\n",
    "SMOTE_svm_df = pd.DataFrame()\n",
    "SMOTE_svm_df.astype('float')\n",
    "SMOTE_svm_pipe = Pipeline(give_me_pipe(SMOTE(random_state = 0, sampling_strategy=0.2), None, svm2))\n",
    "temp = cross_validator(SMOTE_svm_pipe)\n",
    "SMOTE_svm_cv_100 = temp[0]['test_f1']\n",
    "scores = temp[0]\n",
    "accuracy = (np.mean(scores['test_accuracy']), np.std(scores['test_accuracy']))\n",
    "precision = (np.mean(scores['test_precision']), np.std(scores['test_precision']))\n",
    "recall = (np.mean(scores['test_recall']), np.std(scores['test_recall']))\n",
    "f1 = (np.mean(scores['test_f1']), np.std(scores['test_f1']))\n",
    "auc = (np.mean(scores['test_AUC']), np.std(scores['test_AUC']))\n",
    "SMOTE_svm_df = pd.concat([SMOTE_svm_df, pd.DataFrame.from_records([{\n",
    "    'Accuracy':'%.3f' % accuracy[0],\n",
    "    'Precision':'%.3f' % precision[0],\n",
    "    'Recall':'%.3f' % recall[0],\n",
    "    'F1-score':'%.3f' % f1[0],\n",
    "    'AUC':'%.3f' % auc[0]\n",
    "    ,'TIME' : '%.3f'% temp[1]\n",
    "}])], ignore_index=True)\n",
    "display(SMOTE_svm_df)\n",
    "\n",
    "# SMOTE rf\n",
    "SMOTE_rf_cv_100 = []\n",
    "rf2 = copy.deepcopy(rf)\n",
    "SMOTE_rf_df = pd.DataFrame()\n",
    "SMOTE_rf_df.astype('float')\n",
    "SMOTE_rf_pipe = Pipeline(give_me_pipe(SMOTE(random_state = 0, sampling_strategy=0.2), None, rf2))\n",
    "temp = cross_validator(SMOTE_rf_pipe)\n",
    "SMOTE_rf_cv_100 = temp[0]['test_f1']\n",
    "scores = temp[0]\n",
    "accuracy = (np.mean(scores['test_accuracy']), np.std(scores['test_accuracy']))\n",
    "precision = (np.mean(scores['test_precision']), np.std(scores['test_precision']))\n",
    "recall = (np.mean(scores['test_recall']), np.std(scores['test_recall']))\n",
    "f1 = (np.mean(scores['test_f1']), np.std(scores['test_f1']))\n",
    "auc = (np.mean(scores['test_AUC']), np.std(scores['test_AUC']))\n",
    "SMOTE_rf_df = pd.concat([SMOTE_rf_df, pd.DataFrame.from_records([{\n",
    "    'Accuracy':'%.3f' % accuracy[0],\n",
    "    'Precision':'%.3f' % precision[0],\n",
    "    'Recall':'%.3f' % recall[0],\n",
    "    'F1-score':'%.3f' % f1[0],\n",
    "    'AUC':'%.3f' % auc[0],'TIME' : '%.3f'% temp[1]\n",
    "}])], ignore_index=True)\n",
    "display(SMOTE_rf_df)\n",
    "\n",
    "# SMOTE xgb\n",
    "SMOTE_xgb_cv_100 = []\n",
    "xgb2 = copy.deepcopy(xgb)\n",
    "SMOTE_xgb_df = pd.DataFrame()\n",
    "SMOTE_xgb_df.astype('float')\n",
    "SMOTE_xgb_pipe = Pipeline(give_me_pipe(SMOTE(random_state = 0, sampling_strategy=0.3), None, xgb2))\n",
    "temp = cross_validator(SMOTE_xgb_pipe)\n",
    "SMOTE_xgb_cv_100 = temp[0]['test_f1']\n",
    "scores = temp[0]\n",
    "accuracy = (np.mean(scores['test_accuracy']), np.std(scores['test_accuracy']))\n",
    "precision = (np.mean(scores['test_precision']), np.std(scores['test_precision']))\n",
    "recall = (np.mean(scores['test_recall']), np.std(scores['test_recall']))\n",
    "f1 = (np.mean(scores['test_f1']), np.std(scores['test_f1']))\n",
    "auc = (np.mean(scores['test_AUC']), np.std(scores['test_AUC']))\n",
    "SMOTE_xgb_df = pd.concat([SMOTE_xgb_df, pd.DataFrame.from_records([{\n",
    "    'Accuracy':'%.3f' % accuracy[0],\n",
    "    'Precision':'%.3f' % precision[0],\n",
    "    'Recall':'%.3f' % recall[0],\n",
    "    'F1-score':'%.3f' % f1[0],\n",
    "    'AUC':'%.3f' % auc[0],'TIME' : '%.3f'% temp[1]\n",
    "}])], ignore_index=True)\n",
    "display(SMOTE_xgb_df)\n",
    "\n",
    "# lr_cv_100\n",
    "# SMOTE_lr_cv_100\n",
    "a = [lr_cv_100, svm_cv_100, rf_cv_100, xgb_cv_100]\n",
    "b = [SMOTE_lr_cv_100, SMOTE_svm_cv_100, SMOTE_rf_cv_100, SMOTE_xgb_cv_100]\n",
    "for j in np.arange(4):\n",
    "    stat, p = wilcoxon(a[j], b[j])\n",
    "    alpha = 0.05\n",
    "    print(\"%.5f\" % p)\n",
    "    if p > alpha:\n",
    "        print('Same distribution (fail to reject H0)')\n",
    "    else:\n",
    "        print('Different distribution (reject H0)')\n",
    "    d, res = cliffs_delta(a[j], b[j])\n",
    "    print (d, res)\n",
    "    print(\"================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60700037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
